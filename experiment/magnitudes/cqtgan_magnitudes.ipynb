{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "G6ZdrN-wfKlt"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub import AudioSegment\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Check that TensorFlow can see the GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the audio files\n",
    "Run the constant Q transform over the audio files and save as numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make all the clips 3 seconds long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale clips to be about 3 seconds\n",
    "target_duration  = 3\n",
    "\n",
    "# Source File path\n",
    "source = \"./samples/\"\n",
    "\n",
    "# Read the duration of each clip in the source file\n",
    "for i, sample in enumerate(os.listdir(source)):\n",
    "    \n",
    "    try:\n",
    "        # Try to read the sample duration\n",
    "        song = AudioSegment.from_mp3(source+sample)\n",
    "        sample_dur = (song.duration_seconds)\n",
    "\n",
    "        scale_factor = target_duration / sample_dur\n",
    "\n",
    "        # time stretch by 1/scale_factor to get the target sample length\n",
    "        y, sr = librosa.load(source+sample)\n",
    "        y_adjusted = librosa.effects.time_stretch(y, rate=1/scale_factor)\n",
    "\n",
    "        # Write out audio as 24bit PCM WAV\n",
    "        sf.write(f'scaled_samples/{i}.wav', y_adjusted, sr, subtype='PCM_24')\n",
    "    \n",
    "    except:\n",
    "        os.remove(source + sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the sounds to tensors and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_audio_to_complex_array(filename, outfilename=None, overwrite=False):\n",
    "    \"\"\"convert_audio_to_complex_array -- using librosa's short time Fourier transform.\n",
    "    \n",
    "    Arguments:\n",
    "    filename -- filepath to the file that you to copy to an array\n",
    "    outfilename -- filepath to the output array \n",
    "    overwrite -- whether to overwrite if a file already exists with the given outfilename\n",
    "    \n",
    "    Returns -- None\n",
    "    \"\"\"\n",
    "    \n",
    "    audio_data, sr = librosa.load(filename)\n",
    "    \n",
    "    # Get the CQT magnitude, 7 ocatves at 36 bins per octave\n",
    "    # NOTE THERE IS A MIN FREQ SETTING fmin=librosa.note_to_hz('C2'),\n",
    "    \n",
    "    C = np.abs(librosa.cqt(y=audio_data, sr=sr, bins_per_octave=36, n_bins=7*36))\n",
    "    \n",
    "    print(np.shape(C))\n",
    "    np.save(filename[:-4] + \".npy\", C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the files to numpy arrays and save\n",
    "total = len(os.listdir(\"samples/\"))\n",
    "\n",
    "for i, item in enumerate(os.listdir(\"samples/\")):\n",
    "    convert_audio_to_complex_array(\"samples/\"+item)\n",
    "    print(f\"{i+1} out of {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the mp3 files\n",
    "for i, item in enumerate(os.listdir(\"samples/\")):\n",
    "    if item.endswith(\".mp3\") or item.endswith(\".wav\"):\n",
    "        os.remove(\"samples/\" + item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all the files into a single tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the mega tensor\n",
    "\n",
    "target_len = 128\n",
    "target_height = 256\n",
    "target_samples = len(os.listdir(\"samples/\"))\n",
    "\n",
    "mega_tensor = np.zeros([target_samples, target_height, target_len, 1], dtype = np.float32)\n",
    "\n",
    "# Add every sample to the mega tensor\n",
    "for i, name in enumerate(os.listdir(\"samples/\")):\n",
    "    item = np.load(\"samples/\" + name)\n",
    "    \n",
    "    for j in range(len(item)):\n",
    "        for k in range(len(item[0])):\n",
    "            if k < target_len:\n",
    "                mega_tensor[i][j][k] = item[j][k]\n",
    "                \n",
    "                \n",
    "# Then save the mega tensor\n",
    "np.save(\"data.npy\", mega_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 65536)             8454144   \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 32, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 64, 32, 128)      262272    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 64, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 128, 64, 256)     524544    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 128, 64, 256)      0         \n",
      "                                                                 \n",
      " conv2d_transpose_5 (Conv2DT  (None, 256, 128, 512)    2097664   \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 256, 128, 512)     0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 256, 128, 1)       12801     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,351,425\n",
      "Trainable params: 11,351,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## KERAS MODELS\n",
    "latent_dim = 128\n",
    "\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(latent_dim,)),\n",
    "        layers.Dense(32 * 16 * 128),\n",
    "        layers.Reshape((32, 16, 128)),\n",
    "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(1, kernel_size=5, padding=\"same\", activation=\"tanh\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fF8nkpXQV0Nv",
    "outputId": "94f18c94-fb14-4c3b-fbe0-89ee2ad7516d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 128, 64, 64)       1088      \n",
      "                                                                 \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 128, 64, 64)       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 64, 32, 128)       131200    \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 64, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 32, 16, 128)       262272    \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 32, 16, 128)       0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 65536)             0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 65536)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65537     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 460,097\n",
      "Trainable params: 460,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(256, 128, 1)),\n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeWoIKzCxZM2"
   },
   "source": [
    "# Create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QY0txAxoxiCR",
    "outputId": "e57e2310-a542-4bae-c890-4d07ee50d8e1"
   },
   "outputs": [],
   "source": [
    "# Load dataset from directory with keras\n",
    "mega_tensor =  np.load(\"data.npy\")\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(mega_tensor)\n",
    "dataset = train_ds.batch(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8Ouaru6z6a-"
   },
   "source": [
    "# Training\n",
    "https://towardsdatascience.com/generative-adversarial-network-gan-for-dummies-a-step-by-step-tutorial-fdefff170391"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFApsZvS1yYS"
   },
   "source": [
    "1. Select a number of real images from the training set.\n",
    "2. Generate a number of fake images. This is done by sampling random noise vectors and creating images from them using the generator\n",
    "3. Train the discriminator for one or more epochs using both fake and real images. This will update on the discrimators weights by labeling all the real images as 1 and the fake images as 0.\n",
    "4. Generate another number of fake images\n",
    "5. Train the full GAN model for one or more epochs using only fake images. This will update only the generator's weights by labeling all fake images as 1. \n",
    "\n",
    "**SOURCE**: Link above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xD4nOTZ0biK1"
   },
   "outputs": [],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "        \n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "W_5DX3hbbpwv"
   },
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=256):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            \n",
    "            sample = np.reshape(generated_images[i], (256, 128))\n",
    "            \n",
    "            # Save the numpy array\n",
    "            np.save(f\"output-arrays/epoch_{epoch+1}_sample_{i}.npy\", sample)\n",
    "            \n",
    "            # Save a spectrogram\n",
    "            fig, ax = plt.subplots()\n",
    "            img = librosa.display.specshow(librosa.amplitude_to_db(sample), x_axis='time', y_axis='cqt_note', ax=ax)\n",
    "            ax.set_title('Constant-Q power spectrum')\n",
    "            fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n",
    "            plt.savefig(f\"output-specs/epoch_{epoch+1}_sample_{i}.png\")\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "l1ghK7-CbsU_",
    "outputId": "63304994-ee67-4045-81ee-0bcd66bbbac5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1285/1285 [==============================] - ETA: 0s - d_loss: 0.5564 - g_loss: 1.3907"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tyler\\AppData\\Local\\Temp\\ipykernel_19708\\2602637580.py:19: UserWarning: Frequency axis exceeds Nyquist. Did you remember to set all spectrogram parameters in specshow?\n",
      "  img = librosa.display.specshow(librosa.amplitude_to_db(sample), x_axis='time', y_axis='cqt_note', ax=ax)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1285/1285 [==============================] - 211s 163ms/step - d_loss: 0.5564 - g_loss: 1.3907\n",
      "Epoch 2/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.5627 - g_loss: 1.2404\n",
      "Epoch 3/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.5217 - g_loss: 1.2493\n",
      "Epoch 4/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.5134 - g_loss: 1.3601\n",
      "Epoch 5/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4989 - g_loss: 1.4445\n",
      "Epoch 6/100\n",
      "1285/1285 [==============================] - 208s 162ms/step - d_loss: 0.4989 - g_loss: 1.4026\n",
      "Epoch 7/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.4878 - g_loss: 1.4510\n",
      "Epoch 8/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4728 - g_loss: 1.5131\n",
      "Epoch 9/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4728 - g_loss: 1.4886\n",
      "Epoch 10/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4672 - g_loss: 1.6083\n",
      "Epoch 11/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4410 - g_loss: 1.6171\n",
      "Epoch 12/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4470 - g_loss: 1.6324\n",
      "Epoch 13/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4357 - g_loss: 1.6424\n",
      "Epoch 14/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4296 - g_loss: 1.6679\n",
      "Epoch 15/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4494 - g_loss: 1.5927\n",
      "Epoch 16/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4362 - g_loss: 1.6600\n",
      "Epoch 17/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4274 - g_loss: 1.7015\n",
      "Epoch 18/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4219 - g_loss: 1.6938\n",
      "Epoch 19/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.3991 - g_loss: 1.8384\n",
      "Epoch 20/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.4103 - g_loss: 1.7621\n",
      "Epoch 21/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4118 - g_loss: 1.7725\n",
      "Epoch 22/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4009 - g_loss: 1.7255\n",
      "Epoch 23/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4030 - g_loss: 1.7944\n",
      "Epoch 24/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.3938 - g_loss: 1.7713\n",
      "Epoch 25/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.3946 - g_loss: 1.7897\n",
      "Epoch 26/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4109 - g_loss: 1.7316\n",
      "Epoch 27/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.3786 - g_loss: 1.8486\n",
      "Epoch 28/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4018 - g_loss: 1.7596\n",
      "Epoch 29/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.3797 - g_loss: 1.8594\n",
      "Epoch 30/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.3693 - g_loss: 1.9270\n",
      "Epoch 31/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.3898 - g_loss: 1.8630\n",
      "Epoch 32/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.3796 - g_loss: 1.8435\n",
      "Epoch 33/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.3693 - g_loss: 1.9175\n",
      "Epoch 34/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.3614 - g_loss: 1.9972\n",
      "Epoch 35/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.3714 - g_loss: 1.9530\n",
      "Epoch 36/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.3794 - g_loss: 1.8889\n",
      "Epoch 37/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.3904 - g_loss: 1.8352\n",
      "Epoch 38/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.3938 - g_loss: 1.7491\n",
      "Epoch 39/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4306 - g_loss: 1.6514\n",
      "Epoch 40/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4383 - g_loss: 1.6406\n",
      "Epoch 41/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4704 - g_loss: 1.4486\n",
      "Epoch 42/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.4779 - g_loss: 1.4398\n",
      "Epoch 43/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4524 - g_loss: 1.5730\n",
      "Epoch 44/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4770 - g_loss: 1.3872\n",
      "Epoch 45/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4639 - g_loss: 1.4730\n",
      "Epoch 46/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4649 - g_loss: 1.4430\n",
      "Epoch 47/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.4675 - g_loss: 1.4440\n",
      "Epoch 48/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4780 - g_loss: 1.3941\n",
      "Epoch 49/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4736 - g_loss: 1.3896\n",
      "Epoch 50/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4704 - g_loss: 1.4050\n",
      "Epoch 51/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.4619 - g_loss: 1.4231\n",
      "Epoch 52/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4789 - g_loss: 1.3784\n",
      "Epoch 53/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4614 - g_loss: 1.4246\n",
      "Epoch 54/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4577 - g_loss: 1.4344\n",
      "Epoch 55/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.4669 - g_loss: 1.3988\n",
      "Epoch 56/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4696 - g_loss: 1.3961\n",
      "Epoch 57/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4671 - g_loss: 1.3624\n",
      "Epoch 58/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.4576 - g_loss: 1.4670\n",
      "Epoch 59/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.4651 - g_loss: 1.3784\n",
      "Epoch 60/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.4603 - g_loss: 1.4227\n",
      "Epoch 61/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4559 - g_loss: 1.4218\n",
      "Epoch 62/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4280 - g_loss: 1.6966\n",
      "Epoch 63/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4519 - g_loss: 1.4115\n",
      "Epoch 64/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.4601 - g_loss: 1.3969\n",
      "Epoch 65/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4353 - g_loss: 1.6026\n",
      "Epoch 66/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4543 - g_loss: 1.4414\n",
      "Epoch 67/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4433 - g_loss: 1.5412\n",
      "Epoch 68/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4369 - g_loss: 1.5074\n",
      "Epoch 69/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.4536 - g_loss: 1.4194\n",
      "Epoch 70/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4270 - g_loss: 1.6631\n",
      "Epoch 71/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4456 - g_loss: 1.4820\n",
      "Epoch 72/100\n",
      "1285/1285 [==============================] - 205s 160ms/step - d_loss: 0.4443 - g_loss: 1.4640\n",
      "Epoch 73/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4500 - g_loss: 1.4509\n",
      "Epoch 74/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4318 - g_loss: 1.5375\n",
      "Epoch 75/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.4333 - g_loss: 1.5218\n",
      "Epoch 76/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.4524 - g_loss: 1.4334\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4376 - g_loss: 1.5125\n",
      "Epoch 78/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4438 - g_loss: 1.5092\n",
      "Epoch 79/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4387 - g_loss: 1.5053\n",
      "Epoch 80/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4418 - g_loss: 1.4653\n",
      "Epoch 81/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4429 - g_loss: 1.4834\n",
      "Epoch 82/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4367 - g_loss: 1.5257\n",
      "Epoch 83/100\n",
      "1285/1285 [==============================] - 208s 162ms/step - d_loss: 0.4425 - g_loss: 1.4519\n",
      "Epoch 84/100\n",
      "1285/1285 [==============================] - 209s 162ms/step - d_loss: 0.4389 - g_loss: 1.4989\n",
      "Epoch 85/100\n",
      "1285/1285 [==============================] - 208s 162ms/step - d_loss: 0.4402 - g_loss: 1.4664\n",
      "Epoch 86/100\n",
      "1285/1285 [==============================] - 210s 163ms/step - d_loss: 0.4043 - g_loss: 1.7122\n",
      "Epoch 87/100\n",
      "1285/1285 [==============================] - 209s 163ms/step - d_loss: 0.4318 - g_loss: 1.4765\n",
      "Epoch 88/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.4366 - g_loss: 1.4951\n",
      "Epoch 89/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.4340 - g_loss: 1.5096\n",
      "Epoch 90/100\n",
      "1285/1285 [==============================] - 209s 163ms/step - d_loss: 0.4334 - g_loss: 1.5145\n",
      "Epoch 91/100\n",
      "1285/1285 [==============================] - 210s 163ms/step - d_loss: 0.4317 - g_loss: 1.4996\n",
      "Epoch 92/100\n",
      "1285/1285 [==============================] - 209s 163ms/step - d_loss: 0.4233 - g_loss: 1.5971\n",
      "Epoch 93/100\n",
      "1285/1285 [==============================] - 208s 162ms/step - d_loss: 0.4404 - g_loss: 1.4550\n",
      "Epoch 94/100\n",
      "1285/1285 [==============================] - 210s 163ms/step - d_loss: 0.4174 - g_loss: 1.6506\n",
      "Epoch 95/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.4333 - g_loss: 1.4671\n",
      "Epoch 96/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.4344 - g_loss: 1.5037\n",
      "Epoch 97/100\n",
      "1285/1285 [==============================] - 206s 160ms/step - d_loss: 0.4190 - g_loss: 1.6062\n",
      "Epoch 98/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4214 - g_loss: 1.5245\n",
      "Epoch 99/100\n",
      "1285/1285 [==============================] - 207s 161ms/step - d_loss: 0.4183 - g_loss: 1.5806\n",
      "Epoch 100/100\n",
      "1285/1285 [==============================] - 206s 161ms/step - d_loss: 0.4331 - g_loss: 1.5159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af456c5850>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100   # In practice, use ~100 epochs\n",
    "\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=128)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
    ")\n",
    "\n",
    "gan.fit(\n",
    "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=2, latent_dim=128)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GAN-Attempt-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
