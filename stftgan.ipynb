{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6ZdrN-wfKlt"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that TensorFlow can see the GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the audio files\n",
    "Run the short time Fourier transform over the audio files and save as numpy arrays in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_audio_to_complex_array(filename, outfilename=None, overwrite=False):\n",
    "    \"\"\"convert_audio_to_complex_array -- using librosa's short time Fourier transform.\n",
    "    \n",
    "    Arguments:\n",
    "    filename -- filepath to the file that you to copy to an array\n",
    "    outfilename -- filepath to the output array \n",
    "    overwrite -- whether to overwrite if a file already exists with the given outfilename\n",
    "    \n",
    "    Returns -- None\n",
    "    \"\"\"\n",
    "    \n",
    "    # sr == sampling rate\n",
    "    audio_data, sr = librosa.load(filename, sr=4096)\n",
    "    \n",
    "    vertical_res = 256\n",
    "    \n",
    "    # Apply the short time Fourier transform\n",
    "    result = librosa.stft(audio_data, center=False, n_fft=vertical_res, win_length=vertical_res)\n",
    "    #print(str(len(result)) + \"   \" + str(len(result[0])))\n",
    "    np.save(filename[:-4] + \".npy\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert all the files to numpy arrays and save\n",
    "for i, item in enumerate(os.listdir(\"samples/\")):\n",
    "    convert_audio_to_complex_array(\"samples/\"+item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the mp3 files\n",
    "for i, item in enumerate(os.listdir(\"samples/\")):\n",
    "    if item.endswith(\".mp3\"):\n",
    "        os.remove(\"samples/\" + item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all the files into a single tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the mega tensor\n",
    "\n",
    "target_len = 312\n",
    "target_height = len(np.load(\"samples/\" +  os.listdir(\"samples/\")[0]))-1\n",
    "target_samples = len(os.listdir(\"samples/\"))\n",
    "channels = 2\n",
    "\n",
    "print(target_height)\n",
    "\n",
    "mega_tensor = np.zeros([target_samples, target_height, target_len, channels], dtype = np.float32)\n",
    "\n",
    "# Add every sample to the mega tensor\n",
    "for i, name in enumerate(os.listdir(\"samples/\")):\n",
    "    item = np.load(\"samples/\" + name)\n",
    "    \n",
    "    for j in range(len(item)-1):\n",
    "        for k in range(len(item[0])):\n",
    "            if k < target_len:\n",
    "                mega_tensor[i][j][k][0] = np.real(item[j][k])\n",
    "                mega_tensor[i][j][k][1] = np.imag(item[j][k])\n",
    "                \n",
    "                \n",
    "# Then save the mega tensor\n",
    "np.save(\"data.npy\", mega_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KERAS MODELS\n",
    "latent_dim = 128\n",
    "\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(latent_dim,)),\n",
    "        layers.Dense(16 * 39 * 128),\n",
    "        layers.Reshape((16, 39, 128)),\n",
    "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(2, kernel_size=5, padding=\"same\", activation=\"tanh\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fF8nkpXQV0Nv",
    "outputId": "94f18c94-fb14-4c3b-fbe0-89ee2ad7516d"
   },
   "outputs": [],
   "source": [
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(128, 312, 2)),\n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeWoIKzCxZM2"
   },
   "source": [
    "# Create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QY0txAxoxiCR",
    "outputId": "e57e2310-a542-4bae-c890-4d07ee50d8e1"
   },
   "outputs": [],
   "source": [
    "# Load dataset from directory with keras\n",
    "mega_tensor =  np.load(\"data.npy\")\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(mega_tensor)\n",
    "dataset = train_ds.batch(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8Ouaru6z6a-"
   },
   "source": [
    "# Training\n",
    "https://towardsdatascience.com/generative-adversarial-network-gan-for-dummies-a-step-by-step-tutorial-fdefff170391"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFApsZvS1yYS"
   },
   "source": [
    "1. Select a number of real images from the training set.\n",
    "2. Generate a number of fake images. This is done by sampling random noise vectors and creating images from them using the generator\n",
    "3. Train the discriminator for one or more epochs using both fake and real images. This will update on the discrimators weights by labeling all the real images as 1 and the fake images as 0.\n",
    "4. Generate another number of fake images\n",
    "5. Train the full GAN model for one or more epochs using only fake images. This will update only the generator's weights by labeling all fake images as 1. \n",
    "\n",
    "**SOURCE**: Link above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xD4nOTZ0biK1"
   },
   "outputs": [],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "        \n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_5DX3hbbpwv"
   },
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            \n",
    "            sample = generated_images[i]\n",
    "            \n",
    "            # Save the numpy array\n",
    "            np.save(\"output-arrays/generated_%03d_%d.npy\" % (epoch, i), sample)\n",
    "            \n",
    "            # Save a spectrogram\n",
    "\n",
    "            \n",
    "            des = np.zeros([128, 312], dtype=np.complex64)\n",
    "\n",
    "            for i in range(len(sample)):\n",
    "                for k in range(len(sample[0])):\n",
    "                    des[i][k] = complex(sample[i][k][0], sample[i][k][1])\n",
    "\n",
    "            res = librosa.istft(des)\n",
    "\n",
    "            # convert the slices to amplitude\n",
    "            sgram_db = librosa.amplitude_to_db(abs(des))\n",
    "\n",
    "            _, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "            librosa.display.specshow(sgram_db, sr=4096, x_axis='time', y_axis='log', ax=ax, cmap='gray')\n",
    "\n",
    "            plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0)\n",
    "\n",
    "            plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0,)\n",
    "            plt.margins(0,0)\n",
    "            plt.savefig(\"output-specs/generated_%03d_%d.png\" % (epoch, i))\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "l1ghK7-CbsU_",
    "outputId": "63304994-ee67-4045-81ee-0bcd66bbbac5"
   },
   "outputs": [],
   "source": [
    "epochs = 300   # In practice, use ~100 epochs\n",
    "\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=128)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
    ")\n",
    "\n",
    "gan.fit(\n",
    "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=1, latent_dim=128)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load(\"500.npy\")\n",
    "\n",
    "# convert back to complex nums\n",
    "np.shape(test)\n",
    "\n",
    "des = np.zeros([128, 312], dtype=np.complex64)\n",
    "\n",
    "for i in range(len(test)):\n",
    "    for k in range(len(test[0])):\n",
    "        des[i][k] = complex(test[i][k][0], test[i][k][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = librosa.istft(des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need IPython.display's Audio widget\n",
    "from IPython.display import Audio\n",
    "\n",
    "Audio(data=res, rate=4096*16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "    \n",
    "# convert the slices to amplitude\n",
    "sgram_db = librosa.amplitude_to_db(abs(des))\n",
    "\n",
    "_, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "librosa.display.specshow(sgram_db, sr=4096, x_axis='time', y_axis='log', ax=ax, cmap='gray')\n",
    "\n",
    "plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0)\n",
    "\n",
    "plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0,)\n",
    "plt.margins(0,0)\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prev = np.load(\"samples/11.npy\")\n",
    "\n",
    "# convert the slices to amplitude\n",
    "sgram_db = librosa.amplitude_to_db(abs(prev))\n",
    "\n",
    "_, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "librosa.display.specshow(sgram_db, sr=4096, x_axis='time', y_axis='log', ax=ax, cmap='gray')\n",
    "\n",
    "plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0)\n",
    "\n",
    "plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0,)\n",
    "plt.margins(0,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(des))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_sound = librosa.istft(prev)\n",
    "\n",
    "\n",
    "# We'll need IPython.display's Audio widget\n",
    "from IPython.display import Audio\n",
    "\n",
    "Audio(data=prev_sound, rate=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prev[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullset=np.load(\"data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = fullset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des = np.zeros([128, 312], dtype=np.complex64)\n",
    "\n",
    "for i in range(len(sample)):\n",
    "    for k in range(len(sample[0])):\n",
    "        des[i][k] = complex(sample[i][k][0], sample[i][k][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = librosa.istft(des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need IPython.display's Audio widget\n",
    "from IPython.display import Audio\n",
    "\n",
    "Audio(data=res, rate=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = fullset[30]\n",
    "des = np.zeros([128, 312], dtype=np.complex64)\n",
    "\n",
    "for i in range(len(sample)):\n",
    "    for k in range(len(sample[0])):\n",
    "        des[i][k] = complex(sample[i][k][0], sample[i][k][1])\n",
    "\n",
    "res = librosa.istft(des)\n",
    "\n",
    "# convert the slices to amplitude\n",
    "sgram_db = librosa.amplitude_to_db(abs(des))\n",
    "\n",
    "_, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "librosa.display.specshow(sgram_db, sr=4096, x_axis='time', y_axis='log', ax=ax, cmap='gray')\n",
    "\n",
    "plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0)\n",
    "\n",
    "plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0,)\n",
    "plt.margins(0,0)\n",
    "\n",
    "# We'll need IPython.display's Audio widget\n",
    "from IPython.display import Audio\n",
    "\n",
    "Audio(data=res, rate=4096)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GAN-Attempt-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
